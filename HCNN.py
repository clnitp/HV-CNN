# -*- coding: utf-8 -*-
"""Copy of Copy of LSTM_Session.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n9c1SsgLKks6UhR5X7iWLOLNMZc2D3wX
"""

#from google.colab import drive
#drive.mount('/content/drive')

import numpy as np
import pandas as pd

data = pd.read_csv('sessions.csv',header='infer')

data.head()

data.shape

session_lengths = data.groupby('session_id').size()

data = data[np.in1d(data.session_id, session_lengths[session_lengths>1].index)]

item_supports = data.groupby('song_id').size()

data = data[np.in1d(data.song_id, item_supports[item_supports>=5].index)]

session_lengths = data.groupby('session_id').size()

data1 = data[np.in1d(data.session_id, session_lengths[session_lengths>=2].index)]

groups = data1.groupby('session_id')
aggregated = groups['song_id'].agg(sequence = lambda x: list(map(str, x)))
init_ts = groups['ts'].min()
users = groups['user_id'].min()  # it's just fast, min doesn't actually make sense
data = aggregated.join(init_ts).join(users)
data.reset_index(inplace=True)
data.head()

from collections import Counter
cnt = Counter()
data.sequence.map(cnt.update);

sequence_length = data.sequence.map(len).values
n_sessions_per_user = data.groupby('user_id').size()

print('Number of items: {}'.format(len(cnt)))
print('Number of users: {}'.format(data.user_id.nunique()))
print('Number of sessions: {}'.format(len(data)) )

print('\nSession length:\n\tAverage: {:.2f}\n\tMedian: {}\n\tMin: {}\n\tMax: {}'.format(
    sequence_length.mean(), 
    np.quantile(sequence_length, 0.5), 
    sequence_length.min(), 
    sequence_length.max()))

print('Sessions per user:\n\tAverage: {:.2f}\n\tMedian: {}\n\tMin: {}\n\tMax: {}'.format(
    n_sessions_per_user.mean(), 
    np.quantile(n_sessions_per_user, 0.5), 
    n_sessions_per_user.min(), 
    n_sessions_per_user.max()))

dictList = data['sequence']
for i in range(0,len(dictList)):
        if len(dictList[i])<6:
            w=6-len(dictList[i])
            dictList[i]=['10000']*w+dictList[i]

data['sequence'] = dictList

data.head()

from nltk.tokenize import sent_tokenize, word_tokenize
import gensim
from gensim.models import Word2Vec

model = gensim.models.Word2Vec(data['sequence'], min_count = 1, vector_size = 50, window = 5, workers = 20)

#find full vocabular
entire_products=[]
for key in model.wv.index_to_key:
    entire_products.append(key)

vectors = model.syn1neg

a = data['sequence']
word2vec_data = []
for i in range(len(a)):
    seq = a[i]
    seq_vector = []
    for j in range(len(seq)):
        item = seq[j]
        index = entire_products.index(item)
        item_vector = vectors[index]
        seq_vector.append(item_vector)
    word2vec_data.append(seq_vector)

data['word2vec_songs'] = word2vec_data

X = []

for i in range(len(word2vec_data)):
    seq = word2vec_data[i]
    if len(seq)>= 5:
        for j in range(0,(len(seq)-5)):  
            X.append(seq[j:j+5])
            #y.append(seq[j+5])

X = np.array(X)

X.shape

aise_hee = data['sequence'].tolist()
y = []
for i in range(len(aise_hee)):
    seq = aise_hee[i]
    if len(seq)>= 5:
        for j in range(0,(len(seq)-5)):  
            #X.append(seq[j:j+5])
            y.append(seq[j+5])

y = list(map(int, y))
label_encoding_data = list(map(int, entire_products))
from sklearn.preprocessing import LabelEncoder
product_label=LabelEncoder()
product_label.fit(label_encoding_data)
y = product_label.transform(y)

type(y)

max(y)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X.reshape(X.shape[0], -1)).reshape(X.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)

# univariate cnn example
from numpy import array
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten,Reshape
from keras.layers.convolutional import Conv1D, Conv2D
from keras.layers.convolutional import MaxPooling1D, MaxPooling2D
import tensorflow as tf
from keras.models import Model 


n_steps=5
n_features=50
batch_size=32
total_vocab=len(entire_products)

# define model
model = Sequential()
model.add(Conv2D(filters=512, kernel_size=(3,50), activation='relu', input_shape=(n_steps, n_features,1),padding='valid'))
model.add(Reshape(target_shape = (model.output.shape[1], model.output.shape[3])))
model.add(MaxPooling1D(pool_size = model.output.shape[1]))
model.add(Flatten())
model.add(Dense(50, activation='relu'))
model.add(Dense(total_vocab, activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.summary()

# fit model
model.fit(X_train, y_train, epochs=25, validation_split=0.2)

_accuracy = []
_val_accuracy = []
_loss = []
_val_loss = []

_val_loss = model.history.history['val_loss']
_loss = model.history.history['loss']
_accuracy = model.history.history['accuracy']
_val_accuracy = model.history.history['val_accuracy']

import matplotlib.pyplot as plt

plt.plot(_loss,label = "Training loss")
plt.plot(_val_loss,label = "Validation loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(loc = "upper right")
plt.savefig("loss.png")
#plt.show()
plt.close()

plt.plot(_accuracy,label = "Training accuracy")
plt.plot(_val_accuracy,label = "Validation accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc = "lower right")
plt.savefig("accuracy.png")
#plt.show()
plt.close()

print(X_train.shape)

print(X_test.shape)

from sklearn.metrics import accuracy_score

def hit_rate_at_1(prediction,actual):
    return accuracy_score(prediction,actual)

# Hit rata at 5 on test data
def hit_rate_at_5(pred,actual):
    predics = []
    for i in range(0, len(pred)):
        predics.append(np.argsort(pred[i])[-5:])
    count = 0
    for i in range(0, len(predics)):
        if actual[i] in predics[i]:
            count = count + 1

    return count/len(actual)

# Hit rate at 10 on test data
def hit_rate_at_10(pred, actual):
    predics = []
    for i in range(0, len(pred)):
        predics.append(np.argsort(pred[i])[-10:])
    count = 0
    for i in range(0, len(predics)):
        if actual[i] in predics[i]:
            count = count + 1
    return count /len(actual)

# Hit rate at 20 on test data
def hit_rate_at_20(pred, actual):
    predics = []
    for i in range(0, len(pred)):
        predics.append(np.argsort(pred[i])[-20:])
    count = 0
    for i in range(0, len(predics)):
        if actual[i] in predics[i]:
            count = count + 1
    return count /len(actual)

def mrr_at_1(ground_truth, prediction):
    """
    Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth.
    :param ground_truth: the ground truth set or sequence
    :param prediction: the predicted set or sequence
    :return: the value of the metric
    """
    rr = 0.
    for index, p in enumerate(prediction):
        if (p == ground_truth[index]):
            rr = rr+1
    ave = rr/(index+1)   
    return ave

def mrr_at_5(ground_truth, pred):
    """
    Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth.
    :param ground_truth: the ground truth set or sequence
    :param prediction: the predicted set or sequence
    :return: the value of the metric
    """
    predics = []
    for i in range(0, len(pred)):
        predics.append(np.argsort(pred[i])[-5:])
    sum_rr = 0.
    rr = 0. 
    for index, p in enumerate(predics):
        if ground_truth[index] in p:
            rank = np.where(ground_truth[index] == p)[0][0]
            rr = 1. / (rank + 1)
            sum_rr = sum_rr + rr   

    return sum_rr / len(ground_truth)

def mrr_at_10(ground_truth, pred):
    """
    Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth.
    :param ground_truth: the ground truth set or sequence
    :param prediction: the predicted set or sequence
    :return: the value of the metric
    """
    predics = []
    for i in range(0, len(pred)):
        predics.append(np.argsort(pred[i])[-10:])
    sum_rr = 0.
    rr = 0. 
    for index, p in enumerate(predics):
        if ground_truth[index] in p:
            rank = np.where(ground_truth[index] == p)[0][0]
            rr = 1. / (rank + 1)
            sum_rr = sum_rr + rr   

    return sum_rr / len(ground_truth)

def mrr_at_20(ground_truth, pred):
    """
    Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth.
    :param ground_truth: the ground truth set or sequence
    :param prediction: the predicted set or sequence
    :return: the value of the metric
    """
    predics = []
    for i in range(0, len(pred)):
        predics.append(np.argsort(pred[i])[-20:])
    sum_rr = 0.
    rr = 0. 
    for index, p in enumerate(predics):
        if ground_truth[index] in p:
            rank = np.where(ground_truth[index] == p)[0][0]
            rr = 1. / (rank + 1)
            sum_rr = sum_rr + rr   

    return sum_rr / len(ground_truth)

# Prediction on test data
def model_predict(model,test_x,test_seq):
    pred=model.predict(x=test_x)
    preddy=np.argmax(a=pred,axis=1)
    #print(pred.shape)
    #print(preddy.shape)



    print(hit_rate_at_1(preddy,test_seq))
    print(hit_rate_at_5(pred, test_seq))
    print(hit_rate_at_10(pred, test_seq))
    print(hit_rate_at_20(pred, test_seq))
    print(mrr_at_1(test_seq, preddy))
    print(mrr_at_5(test_seq, pred))
    print(mrr_at_10(test_seq, pred))
    print(mrr_at_20(test_seq, pred))

#np.argsort(pred[1])[-10:]

#model_predict(model,X_test,y_test)
#model_predict(model,X_test,y_test)
X_test_df1 = X_test[:150000,:,:]
print(X_test_df1.shape)
X_test_df2 = X_test[150000:300000:,:]
print(X_test_df2.shape)
X_test_df3 = X_test[300000:,:,:]
print(X_test_df3.shape)
#X_test_df4 = X_test[550000:,:,:]
#print(X_test_df4.shape)

y_test_df1 = y_test[:150000]
print(y_test_df1.shape)
y_test_df2 = y_test[150000:300000]
print(y_test_df2.shape)
y_test_df3 = y_test[300000:]
print(y_test_df3.shape)
#y_test_df4 = y_test[550000:]
#print(y_test_df4.shape)


print("for first half of text data")
model_predict(model, X_test_df1, y_test_df1)


print("for second half of text data")
model_predict(model, X_test_df2, y_test_df2)

print("for 3rd half of text data")
model_predict(model, X_test_df3, y_test_df3)

#print("for 4th half of text data")
#model_predict(model, X_test_df4, y_test_df4)